{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU","widgets":{"application/vnd.jupyter.widget-state+json":{"0f17478799c84134a04d65c34ec27914":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_0bc195a76558448bbf362145222af3f2","IPY_MODEL_d832d068d65244c3804a39e59755d1d9","IPY_MODEL_f0387d62825545ae85d174ac9ada1aff"],"layout":"IPY_MODEL_a8e26b37522e4df39a2f18eea45fd489"}},"0bc195a76558448bbf362145222af3f2":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_3408fe3ba186472f806e2c7d2e743e53","placeholder":"​","style":"IPY_MODEL_148b73889ad84bdf8cbfb1bf2a5179cf","value":"Downloading: 100%"}},"d832d068d65244c3804a39e59755d1d9":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_6add7355d549408bb6cadabaad97b4da","max":257706,"min":0,"orientation":"horizontal","style":"IPY_MODEL_8e4f0da0a840474f8fe29dd37b50cd1d","value":257706}},"f0387d62825545ae85d174ac9ada1aff":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_21cd779fa4524b129f0e8e9d2cc0f20a","placeholder":"​","style":"IPY_MODEL_320c6dab4dee481f9f54e353ee850d31","value":" 258k/258k [00:00&lt;00:00, 300kB/s]"}},"a8e26b37522e4df39a2f18eea45fd489":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3408fe3ba186472f806e2c7d2e743e53":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"148b73889ad84bdf8cbfb1bf2a5179cf":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"6add7355d549408bb6cadabaad97b4da":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8e4f0da0a840474f8fe29dd37b50cd1d":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"21cd779fa4524b129f0e8e9d2cc0f20a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"320c6dab4dee481f9f54e353ee850d31":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"0fc3aa9171b3442db2c1d55c75dd8bc7":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_a95666a4d616454798eb64e62d5e8e66","IPY_MODEL_e10756814bdd4d9186a7f28b5765d9a5","IPY_MODEL_8ce5f52462504c2584acd620d34aa946"],"layout":"IPY_MODEL_0fe992d406a74ea68ed4ea4744816dd7"}},"a95666a4d616454798eb64e62d5e8e66":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_2689e328bacb46bbae119ee79cc39c6b","placeholder":"​","style":"IPY_MODEL_181255e195a44796ae93811ea732d94b","value":"Downloading: 100%"}},"e10756814bdd4d9186a7f28b5765d9a5":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_9bbd6dcf5a214117b050e213298d9b69","max":479,"min":0,"orientation":"horizontal","style":"IPY_MODEL_c7102132d1ed4205b63d8fe4d7be7311","value":479}},"8ce5f52462504c2584acd620d34aa946":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_85620d69204a4756ac8ee157865b3faf","placeholder":"​","style":"IPY_MODEL_98cecddd60ee4638b3b6a8f825fdf5e3","value":" 479/479 [00:00&lt;00:00, 6.12kB/s]"}},"0fe992d406a74ea68ed4ea4744816dd7":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"2689e328bacb46bbae119ee79cc39c6b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"181255e195a44796ae93811ea732d94b":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"9bbd6dcf5a214117b050e213298d9b69":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c7102132d1ed4205b63d8fe4d7be7311":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"85620d69204a4756ac8ee157865b3faf":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"98cecddd60ee4638b3b6a8f825fdf5e3":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"ecc5500e81c6484283817522b7b34608":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_33259f382348464eaf86768db7b16bd0","IPY_MODEL_3d517be020a94f4c89cfe90d7ddb5eee","IPY_MODEL_372b147b1adc41349ae131a55ad41bf0"],"layout":"IPY_MODEL_ac1b335f8e7b4010bba887ef3a865c07"}},"33259f382348464eaf86768db7b16bd0":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_cea358103a224a74b6f40ff300cf4dad","placeholder":"​","style":"IPY_MODEL_1c446937dd124756a0867362e391ba82","value":"Downloading: 100%"}},"3d517be020a94f4c89cfe90d7ddb5eee":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_783fe5810d2b42978a4758b12179a5df","max":445021143,"min":0,"orientation":"horizontal","style":"IPY_MODEL_132a41bc16834c6f825bf6c1511c908b","value":445021143}},"372b147b1adc41349ae131a55ad41bf0":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_ae7365d99bf5445fb36b98450a3cd722","placeholder":"​","style":"IPY_MODEL_5af4c54b089847d3b4634b8584269fff","value":" 445M/445M [00:08&lt;00:00, 66.3MB/s]"}},"ac1b335f8e7b4010bba887ef3a865c07":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"cea358103a224a74b6f40ff300cf4dad":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"1c446937dd124756a0867362e391ba82":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"783fe5810d2b42978a4758b12179a5df":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"132a41bc16834c6f825bf6c1511c908b":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"ae7365d99bf5445fb36b98450a3cd722":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"5af4c54b089847d3b4634b8584269fff":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"cells":[{"cell_type":"markdown","metadata":{"id":"view-in-github"},"source":["<a href=\"https://colab.research.google.com/github/ZeroxTM/BERT-CNN-Fine-Tuning-For-Hate-Speech-Detection-in-Online-Social-Media/blob/main/BertCnnFinal.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"As9J92OdsAcd","executionInfo":{"status":"ok","timestamp":1670062137007,"user_tz":-540,"elapsed":26899,"user":{"displayName":"岩本陸","userId":"13347060052503904420"}},"outputId":"826528b9-3b09-479f-b615-469ec1104bc0"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","source":["#ライブラリーのインストール\n","!pip install fugashi==1.1.0 ipadic==1.0.0 pytorch-lightning==1.6.1"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"VajNYau-_Qxz","executionInfo":{"status":"ok","timestamp":1670062150076,"user_tz":-540,"elapsed":13073,"user":{"displayName":"岩本陸","userId":"13347060052503904420"}},"outputId":"f2625ae9-3b16-49b7-99f8-7c8048a75fb2"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting fugashi==1.1.0\n","  Downloading fugashi-1.1.0-cp38-cp38-manylinux1_x86_64.whl (490 kB)\n","\u001b[K     |████████████████████████████████| 490 kB 36.1 MB/s \n","\u001b[?25hCollecting ipadic==1.0.0\n","  Downloading ipadic-1.0.0.tar.gz (13.4 MB)\n","\u001b[K     |████████████████████████████████| 13.4 MB 13.4 MB/s \n","\u001b[?25hCollecting pytorch-lightning==1.6.1\n","  Downloading pytorch_lightning-1.6.1-py3-none-any.whl (582 kB)\n","\u001b[K     |████████████████████████████████| 582 kB 71.4 MB/s \n","\u001b[?25hRequirement already satisfied: fsspec[http]!=2021.06.0,>=2021.05.0 in /usr/local/lib/python3.8/dist-packages (from pytorch-lightning==1.6.1) (2022.11.0)\n","Requirement already satisfied: PyYAML>=5.4 in /usr/local/lib/python3.8/dist-packages (from pytorch-lightning==1.6.1) (6.0)\n","Requirement already satisfied: tqdm>=4.41.0 in /usr/local/lib/python3.8/dist-packages (from pytorch-lightning==1.6.1) (4.64.1)\n","Requirement already satisfied: tensorboard>=2.2.0 in /usr/local/lib/python3.8/dist-packages (from pytorch-lightning==1.6.1) (2.9.1)\n","Requirement already satisfied: torch>=1.8.* in /usr/local/lib/python3.8/dist-packages (from pytorch-lightning==1.6.1) (1.12.1+cu113)\n","Collecting pyDeprecate<0.4.0,>=0.3.1\n","  Downloading pyDeprecate-0.3.2-py3-none-any.whl (10 kB)\n","Collecting torchmetrics>=0.4.1\n","  Downloading torchmetrics-0.11.0-py3-none-any.whl (512 kB)\n","\u001b[K     |████████████████████████████████| 512 kB 79.4 MB/s \n","\u001b[?25hRequirement already satisfied: packaging>=17.0 in /usr/local/lib/python3.8/dist-packages (from pytorch-lightning==1.6.1) (21.3)\n","Requirement already satisfied: numpy>=1.17.2 in /usr/local/lib/python3.8/dist-packages (from pytorch-lightning==1.6.1) (1.21.6)\n","Requirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.8/dist-packages (from pytorch-lightning==1.6.1) (4.1.1)\n","Requirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning==1.6.1) (2.23.0)\n","Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.8/dist-packages (from fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning==1.6.1) (3.8.3)\n","Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.8/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning==1.6.1) (4.0.2)\n","Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.8/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning==1.6.1) (22.1.0)\n","Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.8/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning==1.6.1) (1.3.3)\n","Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.8/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning==1.6.1) (1.8.1)\n","Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.8/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning==1.6.1) (1.3.1)\n","Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.8/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning==1.6.1) (6.0.2)\n","Requirement already satisfied: charset-normalizer<3.0,>=2.0 in /usr/local/lib/python3.8/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning==1.6.1) (2.1.1)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from packaging>=17.0->pytorch-lightning==1.6.1) (3.0.9)\n","Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.8/dist-packages (from tensorboard>=2.2.0->pytorch-lightning==1.6.1) (57.4.0)\n","Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.8/dist-packages (from tensorboard>=2.2.0->pytorch-lightning==1.6.1) (1.3.0)\n","Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.8/dist-packages (from tensorboard>=2.2.0->pytorch-lightning==1.6.1) (3.4.1)\n","Requirement already satisfied: grpcio>=1.24.3 in /usr/local/lib/python3.8/dist-packages (from tensorboard>=2.2.0->pytorch-lightning==1.6.1) (1.50.0)\n","Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.8/dist-packages (from tensorboard>=2.2.0->pytorch-lightning==1.6.1) (2.14.1)\n","Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.8/dist-packages (from tensorboard>=2.2.0->pytorch-lightning==1.6.1) (0.6.1)\n","Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.8/dist-packages (from tensorboard>=2.2.0->pytorch-lightning==1.6.1) (0.38.4)\n","Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.8/dist-packages (from tensorboard>=2.2.0->pytorch-lightning==1.6.1) (1.0.1)\n","Requirement already satisfied: protobuf<3.20,>=3.9.2 in /usr/local/lib/python3.8/dist-packages (from tensorboard>=2.2.0->pytorch-lightning==1.6.1) (3.19.6)\n","Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.8/dist-packages (from tensorboard>=2.2.0->pytorch-lightning==1.6.1) (1.8.1)\n","Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.8/dist-packages (from tensorboard>=2.2.0->pytorch-lightning==1.6.1) (0.4.6)\n","Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.8/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=2.2.0->pytorch-lightning==1.6.1) (5.2.0)\n","Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.8/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=2.2.0->pytorch-lightning==1.6.1) (4.9)\n","Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.8/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=2.2.0->pytorch-lightning==1.6.1) (1.15.0)\n","Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.8/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=2.2.0->pytorch-lightning==1.6.1) (0.2.8)\n","Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.8/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=2.2.0->pytorch-lightning==1.6.1) (1.3.1)\n","Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.8/dist-packages (from markdown>=2.6.8->tensorboard>=2.2.0->pytorch-lightning==1.6.1) (4.13.0)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.8/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard>=2.2.0->pytorch-lightning==1.6.1) (3.10.0)\n","Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.8/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard>=2.2.0->pytorch-lightning==1.6.1) (0.4.8)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning==1.6.1) (2022.9.24)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning==1.6.1) (1.24.3)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning==1.6.1) (3.0.4)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning==1.6.1) (2.10)\n","Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.8/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=2.2.0->pytorch-lightning==1.6.1) (3.2.2)\n","Building wheels for collected packages: ipadic\n","  Building wheel for ipadic (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for ipadic: filename=ipadic-1.0.0-py3-none-any.whl size=13556723 sha256=3bf2b1b6af49ef43372ae8eabad138a998654795c8c206cc9216d29dfcea8acc\n","  Stored in directory: /root/.cache/pip/wheels/45/b7/f5/a21e68db846eedcd00d69e37d60bab3f68eb20b1d99cdff652\n","Successfully built ipadic\n","Installing collected packages: torchmetrics, pyDeprecate, pytorch-lightning, ipadic, fugashi\n","Successfully installed fugashi-1.1.0 ipadic-1.0.0 pyDeprecate-0.3.2 pytorch-lightning-1.6.1 torchmetrics-0.11.0\n"]}]},{"cell_type":"code","source":["!apt install aptitude swig\n","!aptitude install mecab libmecab-dev mecab-ipadic-utf8 git make curl xz-utils file -y\n","!pip install mecab-python3==0.996.5\n","!pip install unidic-lite"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"fI2f6rKzA4aY","executionInfo":{"status":"ok","timestamp":1670062218583,"user_tz":-540,"elapsed":68513,"user":{"displayName":"岩本陸","userId":"13347060052503904420"}},"outputId":"f3539b04-4a10-46b5-ebc0-62c2b586d058"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["Reading package lists... Done\n","Building dependency tree       \n","Reading state information... Done\n","The following package was automatically installed and is no longer required:\n","  libnvidia-common-460\n","Use 'apt autoremove' to remove it.\n","The following additional packages will be installed:\n","  aptitude-common libcgi-fast-perl libcgi-pm-perl libclass-accessor-perl\n","  libcwidget3v5 libencode-locale-perl libfcgi-perl libhtml-parser-perl\n","  libhtml-tagset-perl libhttp-date-perl libhttp-message-perl libio-html-perl\n","  libio-string-perl liblwp-mediatypes-perl libparse-debianchangelog-perl\n","  libsigc++-2.0-0v5 libsub-name-perl libtimedate-perl liburi-perl libxapian30\n","  swig3.0\n","Suggested packages:\n","  aptitude-doc-en | aptitude-doc apt-xapian-index debtags tasksel\n","  libcwidget-dev libdata-dump-perl libhtml-template-perl libxml-simple-perl\n","  libwww-perl xapian-tools swig-doc swig-examples swig3.0-examples swig3.0-doc\n","The following NEW packages will be installed:\n","  aptitude aptitude-common libcgi-fast-perl libcgi-pm-perl\n","  libclass-accessor-perl libcwidget3v5 libencode-locale-perl libfcgi-perl\n","  libhtml-parser-perl libhtml-tagset-perl libhttp-date-perl\n","  libhttp-message-perl libio-html-perl libio-string-perl\n","  liblwp-mediatypes-perl libparse-debianchangelog-perl libsigc++-2.0-0v5\n","  libsub-name-perl libtimedate-perl liburi-perl libxapian30 swig swig3.0\n","0 upgraded, 23 newly installed, 0 to remove and 7 not upgraded.\n","Need to get 4,978 kB of archives.\n","After this operation, 21.4 MB of additional disk space will be used.\n","Get:1 http://archive.ubuntu.com/ubuntu bionic/main amd64 aptitude-common all 0.8.10-6ubuntu1 [1,014 kB]\n","Get:2 http://archive.ubuntu.com/ubuntu bionic/main amd64 libsigc++-2.0-0v5 amd64 2.10.0-2 [10.9 kB]\n","Get:3 http://archive.ubuntu.com/ubuntu bionic/main amd64 libcwidget3v5 amd64 0.5.17-7 [286 kB]\n","Get:4 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 libxapian30 amd64 1.4.5-1ubuntu0.1 [631 kB]\n","Get:5 http://archive.ubuntu.com/ubuntu bionic/main amd64 aptitude amd64 0.8.10-6ubuntu1 [1,269 kB]\n","Get:6 http://archive.ubuntu.com/ubuntu bionic/main amd64 libhtml-tagset-perl all 3.20-3 [12.1 kB]\n","Get:7 http://archive.ubuntu.com/ubuntu bionic/main amd64 liburi-perl all 1.73-1 [77.2 kB]\n","Get:8 http://archive.ubuntu.com/ubuntu bionic/main amd64 libhtml-parser-perl amd64 3.72-3build1 [85.9 kB]\n","Get:9 http://archive.ubuntu.com/ubuntu bionic/main amd64 libcgi-pm-perl all 4.38-1 [185 kB]\n","Get:10 http://archive.ubuntu.com/ubuntu bionic/main amd64 libfcgi-perl amd64 0.78-2build1 [32.8 kB]\n","Get:11 http://archive.ubuntu.com/ubuntu bionic/main amd64 libcgi-fast-perl all 1:2.13-1 [9,940 B]\n","Get:12 http://archive.ubuntu.com/ubuntu bionic/main amd64 libsub-name-perl amd64 0.21-1build1 [11.6 kB]\n","Get:13 http://archive.ubuntu.com/ubuntu bionic/main amd64 libclass-accessor-perl all 0.51-1 [21.2 kB]\n","Get:14 http://archive.ubuntu.com/ubuntu bionic/main amd64 libencode-locale-perl all 1.05-1 [12.3 kB]\n","Get:15 http://archive.ubuntu.com/ubuntu bionic/main amd64 libtimedate-perl all 2.3000-2 [37.5 kB]\n","Get:16 http://archive.ubuntu.com/ubuntu bionic/main amd64 libhttp-date-perl all 6.02-1 [10.4 kB]\n","Get:17 http://archive.ubuntu.com/ubuntu bionic/main amd64 libio-html-perl all 1.001-1 [14.9 kB]\n","Get:18 http://archive.ubuntu.com/ubuntu bionic/main amd64 liblwp-mediatypes-perl all 6.02-1 [21.7 kB]\n","Get:19 http://archive.ubuntu.com/ubuntu bionic/main amd64 libhttp-message-perl all 6.14-1 [72.1 kB]\n","Get:20 http://archive.ubuntu.com/ubuntu bionic/main amd64 libio-string-perl all 1.08-3 [11.1 kB]\n","Get:21 http://archive.ubuntu.com/ubuntu bionic/main amd64 libparse-debianchangelog-perl all 1.2.0-12 [49.5 kB]\n","Get:22 http://archive.ubuntu.com/ubuntu bionic/universe amd64 swig3.0 amd64 3.0.12-1 [1,094 kB]\n","Get:23 http://archive.ubuntu.com/ubuntu bionic/universe amd64 swig amd64 3.0.12-1 [6,460 B]\n","Fetched 4,978 kB in 4s (1,378 kB/s)\n","Selecting previously unselected package aptitude-common.\n","(Reading database ... 124015 files and directories currently installed.)\n","Preparing to unpack .../00-aptitude-common_0.8.10-6ubuntu1_all.deb ...\n","Unpacking aptitude-common (0.8.10-6ubuntu1) ...\n","Selecting previously unselected package libsigc++-2.0-0v5:amd64.\n","Preparing to unpack .../01-libsigc++-2.0-0v5_2.10.0-2_amd64.deb ...\n","Unpacking libsigc++-2.0-0v5:amd64 (2.10.0-2) ...\n","Selecting previously unselected package libcwidget3v5:amd64.\n","Preparing to unpack .../02-libcwidget3v5_0.5.17-7_amd64.deb ...\n","Unpacking libcwidget3v5:amd64 (0.5.17-7) ...\n","Selecting previously unselected package libxapian30:amd64.\n","Preparing to unpack .../03-libxapian30_1.4.5-1ubuntu0.1_amd64.deb ...\n","Unpacking libxapian30:amd64 (1.4.5-1ubuntu0.1) ...\n","Selecting previously unselected package aptitude.\n","Preparing to unpack .../04-aptitude_0.8.10-6ubuntu1_amd64.deb ...\n","Unpacking aptitude (0.8.10-6ubuntu1) ...\n","Selecting previously unselected package libhtml-tagset-perl.\n","Preparing to unpack .../05-libhtml-tagset-perl_3.20-3_all.deb ...\n","Unpacking libhtml-tagset-perl (3.20-3) ...\n","Selecting previously unselected package liburi-perl.\n","Preparing to unpack .../06-liburi-perl_1.73-1_all.deb ...\n","Unpacking liburi-perl (1.73-1) ...\n","Selecting previously unselected package libhtml-parser-perl.\n","Preparing to unpack .../07-libhtml-parser-perl_3.72-3build1_amd64.deb ...\n","Unpacking libhtml-parser-perl (3.72-3build1) ...\n","Selecting previously unselected package libcgi-pm-perl.\n","Preparing to unpack .../08-libcgi-pm-perl_4.38-1_all.deb ...\n","Unpacking libcgi-pm-perl (4.38-1) ...\n","Selecting previously unselected package libfcgi-perl.\n","Preparing to unpack .../09-libfcgi-perl_0.78-2build1_amd64.deb ...\n","Unpacking libfcgi-perl (0.78-2build1) ...\n","Selecting previously unselected package libcgi-fast-perl.\n","Preparing to unpack .../10-libcgi-fast-perl_1%3a2.13-1_all.deb ...\n","Unpacking libcgi-fast-perl (1:2.13-1) ...\n","Selecting previously unselected package libsub-name-perl.\n","Preparing to unpack .../11-libsub-name-perl_0.21-1build1_amd64.deb ...\n","Unpacking libsub-name-perl (0.21-1build1) ...\n","Selecting previously unselected package libclass-accessor-perl.\n","Preparing to unpack .../12-libclass-accessor-perl_0.51-1_all.deb ...\n","Unpacking libclass-accessor-perl (0.51-1) ...\n","Selecting previously unselected package libencode-locale-perl.\n","Preparing to unpack .../13-libencode-locale-perl_1.05-1_all.deb ...\n","Unpacking libencode-locale-perl (1.05-1) ...\n","Selecting previously unselected package libtimedate-perl.\n","Preparing to unpack .../14-libtimedate-perl_2.3000-2_all.deb ...\n","Unpacking libtimedate-perl (2.3000-2) ...\n","Selecting previously unselected package libhttp-date-perl.\n","Preparing to unpack .../15-libhttp-date-perl_6.02-1_all.deb ...\n","Unpacking libhttp-date-perl (6.02-1) ...\n","Selecting previously unselected package libio-html-perl.\n","Preparing to unpack .../16-libio-html-perl_1.001-1_all.deb ...\n","Unpacking libio-html-perl (1.001-1) ...\n","Selecting previously unselected package liblwp-mediatypes-perl.\n","Preparing to unpack .../17-liblwp-mediatypes-perl_6.02-1_all.deb ...\n","Unpacking liblwp-mediatypes-perl (6.02-1) ...\n","Selecting previously unselected package libhttp-message-perl.\n","Preparing to unpack .../18-libhttp-message-perl_6.14-1_all.deb ...\n","Unpacking libhttp-message-perl (6.14-1) ...\n","Selecting previously unselected package libio-string-perl.\n","Preparing to unpack .../19-libio-string-perl_1.08-3_all.deb ...\n","Unpacking libio-string-perl (1.08-3) ...\n","Selecting previously unselected package libparse-debianchangelog-perl.\n","Preparing to unpack .../20-libparse-debianchangelog-perl_1.2.0-12_all.deb ...\n","Unpacking libparse-debianchangelog-perl (1.2.0-12) ...\n","Selecting previously unselected package swig3.0.\n","Preparing to unpack .../21-swig3.0_3.0.12-1_amd64.deb ...\n","Unpacking swig3.0 (3.0.12-1) ...\n","Selecting previously unselected package swig.\n","Preparing to unpack .../22-swig_3.0.12-1_amd64.deb ...\n","Unpacking swig (3.0.12-1) ...\n","Setting up libhtml-tagset-perl (3.20-3) ...\n","Setting up libxapian30:amd64 (1.4.5-1ubuntu0.1) ...\n","Setting up swig3.0 (3.0.12-1) ...\n","Setting up libencode-locale-perl (1.05-1) ...\n","Setting up libtimedate-perl (2.3000-2) ...\n","Setting up libio-html-perl (1.001-1) ...\n","Setting up aptitude-common (0.8.10-6ubuntu1) ...\n","Setting up liblwp-mediatypes-perl (6.02-1) ...\n","Setting up liburi-perl (1.73-1) ...\n","Setting up libhtml-parser-perl (3.72-3build1) ...\n","Setting up libcgi-pm-perl (4.38-1) ...\n","Setting up libio-string-perl (1.08-3) ...\n","Setting up libsub-name-perl (0.21-1build1) ...\n","Setting up libfcgi-perl (0.78-2build1) ...\n","Setting up libsigc++-2.0-0v5:amd64 (2.10.0-2) ...\n","Setting up libclass-accessor-perl (0.51-1) ...\n","Setting up swig (3.0.12-1) ...\n","Setting up libhttp-date-perl (6.02-1) ...\n","Setting up libcgi-fast-perl (1:2.13-1) ...\n","Setting up libparse-debianchangelog-perl (1.2.0-12) ...\n","Setting up libhttp-message-perl (6.14-1) ...\n","Setting up libcwidget3v5:amd64 (0.5.17-7) ...\n","Setting up aptitude (0.8.10-6ubuntu1) ...\n","update-alternatives: using /usr/bin/aptitude-curses to provide /usr/bin/aptitude (aptitude) in auto mode\n","Processing triggers for man-db (2.8.3-2ubuntu0.1) ...\n","Processing triggers for libc-bin (2.27-3ubuntu1.6) ...\n","git is already installed at the requested version (1:2.17.1-1ubuntu0.13)\n","make is already installed at the requested version (4.1-9.1ubuntu1)\n","curl is already installed at the requested version (7.58.0-2ubuntu3.21)\n","xz-utils is already installed at the requested version (5.2.2-1.3ubuntu0.1)\n","git is already installed at the requested version (1:2.17.1-1ubuntu0.13)\n","make is already installed at the requested version (4.1-9.1ubuntu1)\n","curl is already installed at the requested version (7.58.0-2ubuntu3.21)\n","xz-utils is already installed at the requested version (5.2.2-1.3ubuntu0.1)\n","The following NEW packages will be installed:\n","  file libmagic-mgc{a} libmagic1{a} libmecab-dev libmecab2{a} mecab mecab-ipadic{a} mecab-ipadic-utf8 mecab-jumandic{a} mecab-jumandic-utf8{a} mecab-utils{a} \n","The following packages will be REMOVED:\n","  libnvidia-common-460{u} \n","0 packages upgraded, 11 newly installed, 1 to remove and 7 not upgraded.\n","Need to get 29.3 MB of archives. After unpacking 282 MB will be used.\n","Get: 1 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 libmagic-mgc amd64 1:5.32-2ubuntu0.4 [184 kB]\n","Get: 2 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 libmagic1 amd64 1:5.32-2ubuntu0.4 [68.6 kB]\n","Get: 3 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 file amd64 1:5.32-2ubuntu0.4 [22.1 kB]\n","Get: 4 http://archive.ubuntu.com/ubuntu bionic/universe amd64 libmecab2 amd64 0.996-5 [257 kB]\n","Get: 5 http://archive.ubuntu.com/ubuntu bionic/universe amd64 libmecab-dev amd64 0.996-5 [308 kB]\n","Get: 6 http://archive.ubuntu.com/ubuntu bionic/universe amd64 mecab-utils amd64 0.996-5 [4,856 B]\n","Get: 7 http://archive.ubuntu.com/ubuntu bionic/universe amd64 mecab-jumandic-utf8 all 7.0-20130310-4 [16.2 MB]\n","Get: 8 http://archive.ubuntu.com/ubuntu bionic/universe amd64 mecab-jumandic all 7.0-20130310-4 [2,212 B]\n","Get: 9 http://archive.ubuntu.com/ubuntu bionic/universe amd64 mecab-ipadic all 2.7.0-20070801+main-1 [12.1 MB]\n","Get: 10 http://archive.ubuntu.com/ubuntu bionic/universe amd64 mecab amd64 0.996-5 [132 kB]\n","Get: 11 http://archive.ubuntu.com/ubuntu bionic/universe amd64 mecab-ipadic-utf8 all 2.7.0-20070801+main-1 [3,522 B]\n","Fetched 29.3 MB in 5s (5,766 kB/s)\n","(Reading database ... 125265 files and directories currently installed.)\n","Removing libnvidia-common-460 (460.106.00-0ubuntu1) ...\n","Selecting previously unselected package libmagic-mgc.\n","(Reading database ... 125260 files and directories currently installed.)\n","Preparing to unpack .../00-libmagic-mgc_1%3a5.32-2ubuntu0.4_amd64.deb ...\n","Unpacking libmagic-mgc (1:5.32-2ubuntu0.4) ...\n","Selecting previously unselected package libmagic1:amd64.\n","Preparing to unpack .../01-libmagic1_1%3a5.32-2ubuntu0.4_amd64.deb ...\n","Unpacking libmagic1:amd64 (1:5.32-2ubuntu0.4) ...\n","Selecting previously unselected package file.\n","Preparing to unpack .../02-file_1%3a5.32-2ubuntu0.4_amd64.deb ...\n","Unpacking file (1:5.32-2ubuntu0.4) ...\n","Selecting previously unselected package libmecab2:amd64.\n","Preparing to unpack .../03-libmecab2_0.996-5_amd64.deb ...\n","Unpacking libmecab2:amd64 (0.996-5) ...\n","Selecting previously unselected package libmecab-dev.\n","Preparing to unpack .../04-libmecab-dev_0.996-5_amd64.deb ...\n","Unpacking libmecab-dev (0.996-5) ...\n","Selecting previously unselected package mecab-utils.\n","Preparing to unpack .../05-mecab-utils_0.996-5_amd64.deb ...\n","Unpacking mecab-utils (0.996-5) ...\n","Selecting previously unselected package mecab-jumandic-utf8.\n","Preparing to unpack .../06-mecab-jumandic-utf8_7.0-20130310-4_all.deb ...\n","Unpacking mecab-jumandic-utf8 (7.0-20130310-4) ...\n","Selecting previously unselected package mecab-jumandic.\n","Preparing to unpack .../07-mecab-jumandic_7.0-20130310-4_all.deb ...\n","Unpacking mecab-jumandic (7.0-20130310-4) ...\n","Selecting previously unselected package mecab-ipadic.\n","Preparing to unpack .../08-mecab-ipadic_2.7.0-20070801+main-1_all.deb ...\n","Unpacking mecab-ipadic (2.7.0-20070801+main-1) ...\n","Selecting previously unselected package mecab.\n","Preparing to unpack .../09-mecab_0.996-5_amd64.deb ...\n","Unpacking mecab (0.996-5) ...\n","Selecting previously unselected package mecab-ipadic-utf8.\n","Preparing to unpack .../10-mecab-ipadic-utf8_2.7.0-20070801+main-1_all.deb ...\n","Unpacking mecab-ipadic-utf8 (2.7.0-20070801+main-1) ...\n","Setting up libmecab2:amd64 (0.996-5) ...\n","Setting up libmagic-mgc (1:5.32-2ubuntu0.4) ...\n","Setting up libmagic1:amd64 (1:5.32-2ubuntu0.4) ...\n","Setting up mecab-utils (0.996-5) ...\n","Setting up mecab-ipadic (2.7.0-20070801+main-1) ...\n","Compiling IPA dictionary for Mecab.  This takes long time...\n","reading /usr/share/mecab/dic/ipadic/unk.def ... 40\n","emitting double-array: 100% |###########################################| \n","/usr/share/mecab/dic/ipadic/model.def is not found. skipped.\n","reading /usr/share/mecab/dic/ipadic/Filler.csv ... 19\n","reading /usr/share/mecab/dic/ipadic/Prefix.csv ... 221\n","reading /usr/share/mecab/dic/ipadic/Noun.place.csv ... 72999\n","reading /usr/share/mecab/dic/ipadic/Noun.proper.csv ... 27327\n","reading /usr/share/mecab/dic/ipadic/Noun.csv ... 60477\n","reading /usr/share/mecab/dic/ipadic/Adverb.csv ... 3032\n","reading /usr/share/mecab/dic/ipadic/Noun.nai.csv ... 42\n","reading /usr/share/mecab/dic/ipadic/Noun.name.csv ... 34202\n","reading /usr/share/mecab/dic/ipadic/Symbol.csv ... 208\n","reading /usr/share/mecab/dic/ipadic/Noun.demonst.csv ... 120\n","reading /usr/share/mecab/dic/ipadic/Noun.adverbal.csv ... 795\n","reading /usr/share/mecab/dic/ipadic/Adj.csv ... 27210\n","reading /usr/share/mecab/dic/ipadic/Interjection.csv ... 252\n","reading /usr/share/mecab/dic/ipadic/Noun.adjv.csv ... 3328\n","reading /usr/share/mecab/dic/ipadic/Adnominal.csv ... 135\n","reading /usr/share/mecab/dic/ipadic/Postp.csv ... 146\n","reading /usr/share/mecab/dic/ipadic/Others.csv ... 2\n","reading /usr/share/mecab/dic/ipadic/Noun.others.csv ... 151\n","reading /usr/share/mecab/dic/ipadic/Auxil.csv ... 199\n","reading /usr/share/mecab/dic/ipadic/Suffix.csv ... 1393\n","reading /usr/share/mecab/dic/ipadic/Conjunction.csv ... 171\n","reading /usr/share/mecab/dic/ipadic/Noun.verbal.csv ... 12146\n","reading /usr/share/mecab/dic/ipadic/Postp-col.csv ... 91\n","reading /usr/share/mecab/dic/ipadic/Noun.number.csv ... 42\n","reading /usr/share/mecab/dic/ipadic/Verb.csv ... 130750\n","reading /usr/share/mecab/dic/ipadic/Noun.org.csv ... 16668\n","emitting double-array: 100% |###########################################| \n","reading /usr/share/mecab/dic/ipadic/matrix.def ... 1316x1316\n","emitting matrix      : 100% |###########################################| \n","\n","done!\n","update-alternatives: using /var/lib/mecab/dic/ipadic to provide /var/lib/mecab/dic/debian (mecab-dictionary) in auto mode\n","Setting up libmecab-dev (0.996-5) ...\n","Setting up file (1:5.32-2ubuntu0.4) ...\n","Setting up mecab-jumandic-utf8 (7.0-20130310-4) ...\n","Compiling Juman dictionary for Mecab.\n","reading /usr/share/mecab/dic/juman/unk.def ... 37\n","emitting double-array: 100% |###########################################| \n","reading /usr/share/mecab/dic/juman/Emoticon.csv ... 972\n","reading /usr/share/mecab/dic/juman/Prefix.csv ... 90\n","reading /usr/share/mecab/dic/juman/Assert.csv ... 34\n","reading /usr/share/mecab/dic/juman/Noun.koyuu.csv ... 7964\n","reading /usr/share/mecab/dic/juman/Demonstrative.csv ... 97\n","reading /usr/share/mecab/dic/juman/Auto.csv ... 18931\n","reading /usr/share/mecab/dic/juman/Noun.hukusi.csv ... 81\n","reading /usr/share/mecab/dic/juman/Noun.suusi.csv ... 49\n","reading /usr/share/mecab/dic/juman/Wikipedia.csv ... 167709\n","reading /usr/share/mecab/dic/juman/AuxV.csv ... 593\n","reading /usr/share/mecab/dic/juman/Rengo.csv ... 1118\n","reading /usr/share/mecab/dic/juman/Postp.csv ... 108\n","reading /usr/share/mecab/dic/juman/Suffix.csv ... 2128\n","reading /usr/share/mecab/dic/juman/Special.csv ... 158\n","reading /usr/share/mecab/dic/juman/Noun.keishiki.csv ... 8\n","reading /usr/share/mecab/dic/juman/ContentW.csv ... 551145\n","emitting double-array: 100% |###########################################| \n","reading /usr/share/mecab/dic/juman/matrix.def ... 1876x1876\n","emitting matrix      : 100% |###########################################| \n","\n","done!\n","Setting up mecab-ipadic-utf8 (2.7.0-20070801+main-1) ...\n","Compiling IPA dictionary for Mecab.  This takes long time...\n","reading /usr/share/mecab/dic/ipadic/unk.def ... 40\n","emitting double-array: 100% |###########################################| \n","/usr/share/mecab/dic/ipadic/model.def is not found. skipped.\n","reading /usr/share/mecab/dic/ipadic/Filler.csv ... 19\n","reading /usr/share/mecab/dic/ipadic/Prefix.csv ... 221\n","reading /usr/share/mecab/dic/ipadic/Noun.place.csv ... 72999\n","reading /usr/share/mecab/dic/ipadic/Noun.proper.csv ... 27327\n","reading /usr/share/mecab/dic/ipadic/Noun.csv ... 60477\n","reading /usr/share/mecab/dic/ipadic/Adverb.csv ... 3032\n","reading /usr/share/mecab/dic/ipadic/Noun.nai.csv ... 42\n","reading /usr/share/mecab/dic/ipadic/Noun.name.csv ... 34202\n","reading /usr/share/mecab/dic/ipadic/Symbol.csv ... 208\n","reading /usr/share/mecab/dic/ipadic/Noun.demonst.csv ... 120\n","reading /usr/share/mecab/dic/ipadic/Noun.adverbal.csv ... 795\n","reading /usr/share/mecab/dic/ipadic/Adj.csv ... 27210\n","reading /usr/share/mecab/dic/ipadic/Interjection.csv ... 252\n","reading /usr/share/mecab/dic/ipadic/Noun.adjv.csv ... 3328\n","reading /usr/share/mecab/dic/ipadic/Adnominal.csv ... 135\n","reading /usr/share/mecab/dic/ipadic/Postp.csv ... 146\n","reading /usr/share/mecab/dic/ipadic/Others.csv ... 2\n","reading /usr/share/mecab/dic/ipadic/Noun.others.csv ... 151\n","reading /usr/share/mecab/dic/ipadic/Auxil.csv ... 199\n","reading /usr/share/mecab/dic/ipadic/Suffix.csv ... 1393\n","reading /usr/share/mecab/dic/ipadic/Conjunction.csv ... 171\n","reading /usr/share/mecab/dic/ipadic/Noun.verbal.csv ... 12146\n","reading /usr/share/mecab/dic/ipadic/Postp-col.csv ... 91\n","reading /usr/share/mecab/dic/ipadic/Noun.number.csv ... 42\n","reading /usr/share/mecab/dic/ipadic/Verb.csv ... 130750\n","reading /usr/share/mecab/dic/ipadic/Noun.org.csv ... 16668\n","emitting double-array: 100% |###########################################| \n","reading /usr/share/mecab/dic/ipadic/matrix.def ... 1316x1316\n","emitting matrix      : 100% |###########################################| \n","\n","done!\n","update-alternatives: using /var/lib/mecab/dic/ipadic-utf8 to provide /var/lib/mecab/dic/debian (mecab-dictionary) in auto mode\n","Setting up mecab (0.996-5) ...\n","Compiling IPA dictionary for Mecab.  This takes long time...\n","reading /usr/share/mecab/dic/ipadic/unk.def ... 40\n","emitting double-array: 100% |###########################################| \n","/usr/share/mecab/dic/ipadic/model.def is not found. skipped.\n","reading /usr/share/mecab/dic/ipadic/Filler.csv ... 19\n","reading /usr/share/mecab/dic/ipadic/Prefix.csv ... 221\n","reading /usr/share/mecab/dic/ipadic/Noun.place.csv ... 72999\n","reading /usr/share/mecab/dic/ipadic/Noun.proper.csv ... 27327\n","reading /usr/share/mecab/dic/ipadic/Noun.csv ... 60477\n","reading /usr/share/mecab/dic/ipadic/Adverb.csv ... 3032\n","reading /usr/share/mecab/dic/ipadic/Noun.nai.csv ... 42\n","reading /usr/share/mecab/dic/ipadic/Noun.name.csv ... 34202\n","reading /usr/share/mecab/dic/ipadic/Symbol.csv ... 208\n","reading /usr/share/mecab/dic/ipadic/Noun.demonst.csv ... 120\n","reading /usr/share/mecab/dic/ipadic/Noun.adverbal.csv ... 795\n","reading /usr/share/mecab/dic/ipadic/Adj.csv ... 27210\n","reading /usr/share/mecab/dic/ipadic/Interjection.csv ... 252\n","reading /usr/share/mecab/dic/ipadic/Noun.adjv.csv ... 3328\n","reading /usr/share/mecab/dic/ipadic/Adnominal.csv ... 135\n","reading /usr/share/mecab/dic/ipadic/Postp.csv ... 146\n","reading /usr/share/mecab/dic/ipadic/Others.csv ... 2\n","reading /usr/share/mecab/dic/ipadic/Noun.others.csv ... 151\n","reading /usr/share/mecab/dic/ipadic/Auxil.csv ... 199\n","reading /usr/share/mecab/dic/ipadic/Suffix.csv ... 1393\n","reading /usr/share/mecab/dic/ipadic/Conjunction.csv ... 171\n","reading /usr/share/mecab/dic/ipadic/Noun.verbal.csv ... 12146\n","reading /usr/share/mecab/dic/ipadic/Postp-col.csv ... 91\n","reading /usr/share/mecab/dic/ipadic/Noun.number.csv ... 42\n","reading /usr/share/mecab/dic/ipadic/Verb.csv ... 130750\n","reading /usr/share/mecab/dic/ipadic/Noun.org.csv ... 16668\n","emitting double-array: 100% |###########################################| \n","reading /usr/share/mecab/dic/ipadic/matrix.def ... 1316x1316\n","emitting matrix      : 100% |###########################################| \n","\n","done!\n","Setting up mecab-jumandic (7.0-20130310-4) ...\n","Processing triggers for man-db (2.8.3-2ubuntu0.1) ...\n","Processing triggers for libc-bin (2.27-3ubuntu1.6) ...\n","                            \n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting mecab-python3==0.996.5\n","  Downloading mecab_python3-0.996.5-cp38-cp38-manylinux2010_x86_64.whl (17.1 MB)\n","\u001b[K     |████████████████████████████████| 17.1 MB 498 kB/s \n","\u001b[?25hInstalling collected packages: mecab-python3\n","Successfully installed mecab-python3-0.996.5\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting unidic-lite\n","  Downloading unidic-lite-1.0.8.tar.gz (47.4 MB)\n","\u001b[K     |████████████████████████████████| 47.4 MB 1.2 MB/s \n","\u001b[?25hBuilding wheels for collected packages: unidic-lite\n","  Building wheel for unidic-lite (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for unidic-lite: filename=unidic_lite-1.0.8-py3-none-any.whl size=47658837 sha256=e82d53626aa996d8b4832be83db141b1d13a6018700a36f8e9b9204ff5f00e1a\n","  Stored in directory: /root/.cache/pip/wheels/8c/dd/8f/e21fc192dcd38ae31e1185ce4e66e12df4e811e3d469866e15\n","Successfully built unidic-lite\n","Installing collected packages: unidic-lite\n","Successfully installed unidic-lite-1.0.8\n"]}]},{"cell_type":"code","source":["!pip install transformers==3.0.0\n","!pip install emoji==1.7"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"XKQZe1UuEStf","executionInfo":{"status":"ok","timestamp":1670062236094,"user_tz":-540,"elapsed":17528,"user":{"displayName":"岩本陸","userId":"13347060052503904420"}},"outputId":"277dd8c9-9b8c-4feb-f37a-920578efd36a"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting transformers==3.0.0\n","  Downloading transformers-3.0.0-py3-none-any.whl (754 kB)\n","\u001b[K     |████████████████████████████████| 754 kB 15.9 MB/s \n","\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.8/dist-packages (from transformers==3.0.0) (4.64.1)\n","Collecting sentencepiece\n","  Downloading sentencepiece-0.1.97-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n","\u001b[K     |████████████████████████████████| 1.3 MB 64.6 MB/s \n","\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.8/dist-packages (from transformers==3.0.0) (3.8.0)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.8/dist-packages (from transformers==3.0.0) (1.21.6)\n","Collecting tokenizers==0.8.0-rc4\n","  Downloading tokenizers-0.8.0rc4-cp38-cp38-manylinux1_x86_64.whl (3.0 MB)\n","\u001b[K     |████████████████████████████████| 3.0 MB 57.6 MB/s \n","\u001b[?25hCollecting sacremoses\n","  Downloading sacremoses-0.0.53.tar.gz (880 kB)\n","\u001b[K     |████████████████████████████████| 880 kB 64.5 MB/s \n","\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.8/dist-packages (from transformers==3.0.0) (21.3)\n","Requirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from transformers==3.0.0) (2.23.0)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.8/dist-packages (from transformers==3.0.0) (2022.6.2)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from packaging->transformers==3.0.0) (3.0.9)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests->transformers==3.0.0) (3.0.4)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests->transformers==3.0.0) (2022.9.24)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests->transformers==3.0.0) (2.10)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests->transformers==3.0.0) (1.24.3)\n","Requirement already satisfied: six in /usr/local/lib/python3.8/dist-packages (from sacremoses->transformers==3.0.0) (1.15.0)\n","Requirement already satisfied: click in /usr/local/lib/python3.8/dist-packages (from sacremoses->transformers==3.0.0) (7.1.2)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.8/dist-packages (from sacremoses->transformers==3.0.0) (1.2.0)\n","Building wheels for collected packages: sacremoses\n","  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for sacremoses: filename=sacremoses-0.0.53-py3-none-any.whl size=895260 sha256=8944a32b398f46e0e47eedd0066bc9e350c71c1111a31acb2bee63a75e8066b1\n","  Stored in directory: /root/.cache/pip/wheels/82/ab/9b/c15899bf659ba74f623ac776e861cf2eb8608c1825ddec66a4\n","Successfully built sacremoses\n","Installing collected packages: tokenizers, sentencepiece, sacremoses, transformers\n","Successfully installed sacremoses-0.0.53 sentencepiece-0.1.97 tokenizers-0.8.0rc4 transformers-3.0.0\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting emoji==1.7\n","  Downloading emoji-1.7.0.tar.gz (175 kB)\n","\u001b[K     |████████████████████████████████| 175 kB 4.7 MB/s \n","\u001b[?25hBuilding wheels for collected packages: emoji\n","  Building wheel for emoji (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for emoji: filename=emoji-1.7.0-py3-none-any.whl size=171046 sha256=cbf146dc8845270ceb790d12d7b3da830cb7049da497a8d0702391ccd6fec805\n","  Stored in directory: /root/.cache/pip/wheels/5e/8c/80/c3646df8201ba6f5070297fe3779a4b70265d0bfd961c15302\n","Successfully built emoji\n","Installing collected packages: emoji\n","Successfully installed emoji-1.7.0\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000,"referenced_widgets":["0f17478799c84134a04d65c34ec27914","0bc195a76558448bbf362145222af3f2","d832d068d65244c3804a39e59755d1d9","f0387d62825545ae85d174ac9ada1aff","a8e26b37522e4df39a2f18eea45fd489","3408fe3ba186472f806e2c7d2e743e53","148b73889ad84bdf8cbfb1bf2a5179cf","6add7355d549408bb6cadabaad97b4da","8e4f0da0a840474f8fe29dd37b50cd1d","21cd779fa4524b129f0e8e9d2cc0f20a","320c6dab4dee481f9f54e353ee850d31","0fc3aa9171b3442db2c1d55c75dd8bc7","a95666a4d616454798eb64e62d5e8e66","e10756814bdd4d9186a7f28b5765d9a5","8ce5f52462504c2584acd620d34aa946","0fe992d406a74ea68ed4ea4744816dd7","2689e328bacb46bbae119ee79cc39c6b","181255e195a44796ae93811ea732d94b","9bbd6dcf5a214117b050e213298d9b69","c7102132d1ed4205b63d8fe4d7be7311","85620d69204a4756ac8ee157865b3faf","98cecddd60ee4638b3b6a8f825fdf5e3","ecc5500e81c6484283817522b7b34608","33259f382348464eaf86768db7b16bd0","3d517be020a94f4c89cfe90d7ddb5eee","372b147b1adc41349ae131a55ad41bf0","ac1b335f8e7b4010bba887ef3a865c07","cea358103a224a74b6f40ff300cf4dad","1c446937dd124756a0867362e391ba82","783fe5810d2b42978a4758b12179a5df","132a41bc16834c6f825bf6c1511c908b","ae7365d99bf5445fb36b98450a3cd722","5af4c54b089847d3b4634b8584269fff"]},"id":"9KuTl1VnNfsX","outputId":"41312640-eae3-4f25-c00b-ab7dfd287fcf","executionInfo":{"status":"ok","timestamp":1670062469686,"user_tz":-540,"elapsed":233597,"user":{"displayName":"岩本陸","userId":"13347060052503904420"}}},"source":["\"\"\"\n"," @Time : 15/12/2020 19:01\n"," @Author : Alaa Grable\n"," \"\"\"\n","\n","\n","import gc\n","#import os\n","import emoji as emoji\n","import re\n","import string\n","import numpy as np\n","import pandas as pd\n","import torch\n","import torch.nn as nn\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import classification_report, accuracy_score\n","from transformers import AutoModel\n","from transformers import BertModel, BertJapaneseTokenizer\n","\n","class BERT_Arch(nn.Module):\n","\n","    def __init__(self, bert):\n","        super(BERT_Arch, self).__init__()\n","        self.bert = BertModel.from_pretrained('cl-tohoku/bert-base-japanese-whole-word-masking')\n","        self.conv = nn.Conv2d(in_channels=13, out_channels=13, kernel_size=(3, 768), bias=False, padding=1, stride=1,)\n","        self.relu = nn.ReLU()\n","        self.pool = nn.MaxPool2d(kernel_size=3, stride=1)\n","        self.dropout = nn.Dropout(0.1)\n","        self.fc = nn.Linear(1638, 2) # before : 455 with max_length 36 # 819 with max_length 64 #1651 with max_length 128\n","        self.flat = nn.Flatten()\n","        self.softmax = nn.LogSoftmax(dim=1)\n","\n","    def forward(self, sent_id, mask):\n","        sent_id = sent_id.view(-1, 128)\n","        mask = mask.view(-1, 128)\n","        _, _, all_layers = self.bert(sent_id, attention_mask=mask, output_hidden_states=True)\n","        # all_layers  = [13, 64, 128, 768]\n","        x = torch.transpose(torch.cat(tuple([t.unsqueeze(0) for t in all_layers]), 0), 0, 1)\n","        del all_layers\n","        gc.collect()\n","        torch.cuda.empty_cache()\n","        x = self.pool(self.dropout(self.relu(self.conv(self.dropout(x)))))\n","        x = self.fc(self.dropout(self.flat(self.dropout(x))))\n","        return self.softmax(x)\n","\n","\n","def read_dataset():\n","    data = pd.read_csv(\"drive/My Drive/reserch/卒論/BERT-CNN-Fine-Tuning-For-Hate-Speech-Detection-in-Online-Social-Media-main/gunze_labeled_data.csv\")\n","    data = data.drop(['product', 'gender', 'length'], axis=1)\n","    #data = data.loc[0:9599,:]\n","    print(len(data))\n","    return data['review'].tolist(), data['star']\n","\n","\n","def pre_process_dataset(values):\n","    new_values = list()\n","    for value in values:\n","      value = re.sub('<[^>]*>', '', value)\n","      # Save emoticons for later appending\n","      emoticons = re.findall('(?::|;|=)(?:-)?(?:\\)|\\(|D|P)', value)\n","      # Remove any non-word character and append the emoticons,\n","      # removing the nose character for standarization. Convert to lower case\n","      value = (re.sub('[\\W]+', ' ', value.lower()) + ' ' + ' '.join(emoticons).replace('-', ''))\n","      new_values.append(value)\n","    return new_values\n","\n","\n","def data_process(data, labels):\n","    input_ids = []\n","    attention_masks = []\n","    bert_tokenizer = BertJapaneseTokenizer.from_pretrained('cl-tohoku/bert-base-japanese-whole-word-masking')\n","    for sentence in data:\n","        bert_inp = bert_tokenizer.encode_plus(\n","                        sentence,                      \n","                        max_length = 128,           # 文章の長さを固定（Padding/Trancatinating）\n","                        pad_to_max_length = True,# PADDINGで埋める\n","                        return_attention_mask = True,   # Attention maksの作成\n","                        return_tensors = 'pt',     #  Pytorch tensorsで返す\n","                        truncation=True,\n","                   )\n","        input_ids.append(bert_inp['input_ids'])\n","        attention_masks.append(bert_inp['attention_mask'])\n","    #del bert_tokenizer\n","    #gc.collect()\n","    #torch.cuda.empty_cache()\n","    input_ids = np.asarray(input_ids)\n","    attention_masks = np.array(attention_masks)\n","    labels = np.array(labels)\n","    return input_ids, attention_masks, labels\n","\n","\n","def load_and_process():\n","    data, labels = read_dataset()\n","    num_of_labels = len(labels.unique())\n","    input_ids, attention_masks, labels = data_process(pre_process_dataset(data), labels)\n","\n","    return input_ids, attention_masks, labels\n","\n","\n","# function to train the model\n","def train():\n","    model.train()\n","\n","    total_loss, total_accuracy = 0, 0\n","\n","    # empty list to save model predictions\n","    total_preds = []\n","\n","    # iterate over batches\n","    total = len(train_dataloader)\n","    for i, batch in enumerate(train_dataloader):\n","\n","        step = i+1\n","        percent = \"{0:.2f}\".format(100 * (step / float(total)))\n","        lossp = \"{0:.2f}\".format(total_loss/(total*batch_size))\n","        filledLength = int(100 * step // total)\n","        bar = '█' * filledLength + '>'  *(filledLength < 100) + '.' * (99 - filledLength)\n","        print(f'\\rBatch {step}/{total} |{bar}| {percent}% complete, loss={lossp}, accuracy={total_accuracy}', end='')\n","\n","        # push the batch to gpu\n","        batch = [r.to(device) for r in batch]\n","        sent_id, mask, labels = batch\n","        del batch\n","        gc.collect()\n","        torch.cuda.empty_cache()\n","        # clear previously calculated gradients\n","        model.zero_grad()\n","\n","        # get model predictions for the current batch\n","        #sent_id = torch.tensor(sent_id).to(device).long()\n","        preds = model(sent_id, mask)\n","\n","        # compute the loss between actual and predicted values\n","        loss = cross_entropy(preds, labels)\n","\n","        # add on to the total loss\n","        total_loss += float(loss.item())\n","\n","        # backward pass to calculate the gradients\n","        loss.backward()\n","\n","        # clip the the gradients to 1.0. It helps in preventing the exploding gradient problem\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n","\n","        # update parameters\n","        optimizer.step()\n","\n","        # model predictions are stored on GPU. So, push it to CPU\n","        #preds = preds.detach().cpu().numpy()\n","\n","        # append the model predictions\n","        #total_preds.append(preds)\n","        total_preds.append(preds.detach().cpu().numpy())\n","\n","    gc.collect()\n","    torch.cuda.empty_cache()\n","\n","    # compute the training loss of the epoch\n","    avg_loss = total_loss / (len(train_dataloader)*batch_size)\n","\n","    # predictions are in the form of (no. of batches, size of batch, no. of classes).\n","    # reshape the predictions in form of (number of samples, no. of classes)\n","    total_preds = np.concatenate(total_preds, axis=0)\n","\n","    # returns the loss and predictions\n","    return avg_loss, total_preds\n","\n","\n","# function for evaluating the model\n","def evaluate():\n","    print(\"\\n\\nEvaluating...\")\n","\n","    # deactivate dropout layers\n","    model.eval()\n","\n","    total_loss, total_accuracy = 0, 0\n","\n","    # empty list to save the model predictions\n","    total_preds = []\n","\n","    # iterate over batches\n","    total = len(val_dataloader)\n","    for i, batch in enumerate(val_dataloader):\n","        \n","        step = i+1\n","        percent = \"{0:.2f}\".format(100 * (step / float(total)))\n","        lossp = \"{0:.2f}\".format(total_loss/(total*batch_size))\n","        filledLength = int(100 * step // total)\n","        bar = '█' * filledLength + '>' * (filledLength < 100) + '.' * (99 - filledLength)\n","        print(f'\\rBatch {step}/{total} |{bar}| {percent}% complete, loss={lossp}, accuracy={total_accuracy}', end='')\n","\n","        # push the batch to gpu\n","        batch = [t.to(device) for t in batch]\n","\n","        sent_id, mask, labels = batch\n","        del batch\n","        gc.collect()\n","        torch.cuda.empty_cache()\n","        # deactivate autograd\n","        with torch.no_grad():\n","\n","            # model predictions\n","            preds = model(sent_id, mask)\n","\n","            # compute the validation loss between actual and predicted values\n","            loss = cross_entropy(preds, labels)\n","\n","            total_loss += float(loss.item())\n","            #preds = preds.detach().cpu().numpy()\n","\n","            #total_preds.append(preds)\n","            total_preds.append(preds.detach().cpu().numpy())\n","\n","    gc.collect()\n","    torch.cuda.empty_cache()\n","\n","    # compute the validation loss of the epoch\n","    avg_loss = total_loss / (len(val_dataloader)*batch_size)\n","\n","    # reshape the predictions in form of (number of samples, no. of classes)\n","    total_preds = np.concatenate(total_preds, axis=0)\n","\n","    return avg_loss, total_preds\n","\n","# Specify the GPU\n","# Setting up the device for GPU usage\n","device = 'cuda' if torch.cuda.is_available() else 'cpu'\n","print(device)\n","# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ Load Data-set ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n","input_ids, attention_masks, labels = load_and_process()\n","df = pd.DataFrame(list(zip(input_ids, attention_masks)), columns=['input_ids', 'attention_masks'])\n","# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n","# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ class distribution ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n","\n","# class = class label for majority of CF users. 0 - hate speech 1 - offensive language 2 - neither\n","# ~~~~~~~~~~ Split train data-set into train, validation and test sets  ~~~~~~~~~~#\n","train_text, temp_text, train_labels, temp_labels = train_test_split(df, labels,\n","                             random_state=2018, test_size=0.2, stratify=labels)\n","\n","val_text, test_text, val_labels, test_labels = train_test_split(temp_text, temp_labels,\n","                         random_state=2018, test_size=0.5, stratify=temp_labels)\n","\n","del temp_text\n","gc.collect()\n","torch.cuda.empty_cache()\n","\n","train_count = len(train_labels)\n","test_count = len(test_labels)\n","val_count = len(val_labels)\n","# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n","\n","# ~~~~~~~~~~~~~~~~~~~~~ Import BERT Model and BERT Tokenizer ~~~~~~~~~~~~~~~~~~~~~#\n","# import BERT-base pretrained model\n","bert = AutoModel.from_pretrained('cl-tohoku/bert-base-japanese-whole-word-masking')\n","# bert = AutoModel.from_pretrained('bert-base-uncased')\n","# Load the BERT tokenizer\n","#tokenizer = BertTokenizerFast.from_pretrained('bert-base-uncased')\n","# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n","\n","# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ Tokenization ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n","# for train set\n","train_seq = torch.stack(train_text['input_ids'].tolist(), dim=0)\n","train_mask = torch.stack(train_text['attention_masks'].tolist(), dim=0)\n","train_y = torch.tensor(train_labels.tolist())\n","\n","# for validation set\n","val_seq = torch.stack(val_text['input_ids'].tolist(), dim=0)\n","val_mask = torch.stack(val_text['attention_masks'].tolist(), dim=0)\n","val_y = torch.tensor(val_labels.tolist())\n","\n","# for test set\n","test_seq = torch.stack(test_text['input_ids'].tolist(), dim=0)\n","test_mask = torch.stack(test_text['attention_masks'].tolist(), dim=0)\n","test_y = torch.tensor(test_labels.tolist())\n","# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n","\n","# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ Create DataLoaders ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n","from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n","\n","# define a batch size\n","batch_size = 64\n","\n","# wrap tensors\n","train_data = TensorDataset(train_seq, train_mask, train_y)\n","\n","# sampler for sampling the data during training\n","train_sampler = RandomSampler(train_data)\n","\n","# dataLoader for train set\n","train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n","\n","# wrap tensors\n","val_data = TensorDataset(val_seq, val_mask, val_y)\n","\n","# sampler for sampling the data during training\n","val_sampler = SequentialSampler(val_data)\n","\n","# dataLoader for validation set\n","val_dataloader = DataLoader(val_data, sampler=val_sampler, batch_size=batch_size)\n","# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n","\n","# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~ Freeze BERT Parameters ~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n","# freeze all the parameters\n","for param in bert.parameters():\n","    param.requires_grad = False\n","# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n","\n","# pass the pre-trained BERT to our define architecture\n","model = BERT_Arch(bert)\n","# push the model to GPU\n","model = model.to(device)\n","\n","# optimizer from hugging face transformers\n","from transformers import AdamW\n","\n","# define the optimizer\n","optimizer = AdamW(model.parameters(), lr=2e-5)\n","\n","#from sklearn.utils.class_weight import compute_class_weight\n","\n","# compute the class weights\n","#class_wts = compute_class_weight('balanced', np.unique(train_labels), train_labels)\n","\n","#print(class_wts)\n","\n","# convert class weights to tensor\n","#weights = torch.tensor(class_wts, dtype=torch.float)\n","#weights = weights.to(device)\n","\n","# loss function\n","#cross_entropy = nn.NLLLoss(weight=weights)\n","cross_entropy = nn.NLLLoss()\n","\n","# set initial loss to infinite\n","best_valid_loss = float('inf')\n","\n","# empty lists to store training and validation loss of each epoch\n","#train_losses = []\n","#valid_losses = []\n","\n","#if os.path.isfile(\"/content/drive/MyDrive/saved_weights.pth\") == False:\n","#if os.path.isfile(\"saved_weights.pth\") == False:\n","    # number of training epochs\n","epochs = 5\n","current = 1\n","# for each epoch\n","while current <= epochs:\n","\n","    print(f'\\nEpoch {current} / {epochs}:')\n","\n","    # train model\n","    train_loss, _ = train()\n","\n","    # evaluate model\n","    valid_loss, _ = evaluate()\n","\n","    # save the best model\n","    if valid_loss < best_valid_loss:\n","        best_valid_loss = valid_loss\n","        #torch.save(model.state_dict(), 'saved_weights.pth')\n","\n","    # append training and validation loss\n","    #train_losses.append(train_loss)\n","    #valid_losses.append(valid_loss)\n","    print(f'\\n\\nTraining Loss: {train_loss:.3f}')\n","    print(f'Validation Loss: {valid_loss:.3f}')\n","\n","    current = current + 1\n","#else:\n","    #print(\"Got weights!\")\n","    # load weights of best model\n","    #model.load_state_dict(torch.load(\"saved_weights.pth\"))\n","    #model.load_state_dict(torch.load(\"/content/drive/MyDrive/saved_weights.pth\"), strict=False)\n","\n","# get predictions for test data\n","gc.collect()\n","torch.cuda.empty_cache()\n","\n","with torch.no_grad():\n","    preds = model(test_seq.to(device), test_mask.to(device))\n","    #preds = model(test_seq, test_mask)\n","    preds = preds.detach().cpu().numpy()\n","\n","\n","print(\"Performance:\")\n","# model's performance\n","preds = np.argmax(preds, axis=1)\n","print('Classification Report')\n","print(classification_report(test_y, preds))\n","\n","print(\"Accuracy: \" + str(accuracy_score(test_y, preds)))"],"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["cuda\n","1670\n"]},{"output_type":"display_data","data":{"text/plain":["Downloading:   0%|          | 0.00/258k [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0f17478799c84134a04d65c34ec27914"}},"metadata":{}},{"output_type":"stream","name":"stderr","text":["<ipython-input-5-c2a45125ea8b>:87: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.\n","  input_ids = np.asarray(input_ids)\n","<ipython-input-5-c2a45125ea8b>:87: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n","  input_ids = np.asarray(input_ids)\n","<ipython-input-5-c2a45125ea8b>:88: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.\n","  attention_masks = np.array(attention_masks)\n","<ipython-input-5-c2a45125ea8b>:88: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n","  attention_masks = np.array(attention_masks)\n"]},{"output_type":"display_data","data":{"text/plain":["Downloading:   0%|          | 0.00/479 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0fc3aa9171b3442db2c1d55c75dd8bc7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Downloading:   0%|          | 0.00/445M [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ecc5500e81c6484283817522b7b34608"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["\n","Epoch 1 / 5:\n","Batch 21/21 |████████████████████████████████████████████████████████████████████████████████████████████████████| 100.00% complete, loss=0.01, accuracy=0\n","\n","Evaluating...\n","Batch 3/3 |████████████████████████████████████████████████████████████████████████████████████████████████████| 100.00% complete, loss=0.01, accuracy=0\n","\n","Training Loss: 0.011\n","Validation Loss: 0.011\n","\n","Epoch 2 / 5:\n","Batch 21/21 |████████████████████████████████████████████████████████████████████████████████████████████████████| 100.00% complete, loss=0.01, accuracy=0\n","\n","Evaluating...\n","Batch 3/3 |████████████████████████████████████████████████████████████████████████████████████████████████████| 100.00% complete, loss=0.01, accuracy=0\n","\n","Training Loss: 0.010\n","Validation Loss: 0.009\n","\n","Epoch 3 / 5:\n","Batch 21/21 |████████████████████████████████████████████████████████████████████████████████████████████████████| 100.00% complete, loss=0.01, accuracy=0\n","\n","Evaluating...\n","Batch 3/3 |████████████████████████████████████████████████████████████████████████████████████████████████████| 100.00% complete, loss=0.00, accuracy=0\n","\n","Training Loss: 0.007\n","Validation Loss: 0.006\n","\n","Epoch 4 / 5:\n","Batch 21/21 |████████████████████████████████████████████████████████████████████████████████████████████████████| 100.00% complete, loss=0.00, accuracy=0\n","\n","Evaluating...\n","Batch 3/3 |████████████████████████████████████████████████████████████████████████████████████████████████████| 100.00% complete, loss=0.00, accuracy=0\n","\n","Training Loss: 0.005\n","Validation Loss: 0.006\n","\n","Epoch 5 / 5:\n","Batch 21/21 |████████████████████████████████████████████████████████████████████████████████████████████████████| 100.00% complete, loss=0.00, accuracy=0\n","\n","Evaluating...\n","Batch 3/3 |████████████████████████████████████████████████████████████████████████████████████████████████████| 100.00% complete, loss=0.00, accuracy=0\n","\n","Training Loss: 0.003\n","Validation Loss: 0.005\n","Performance:\n","Classification Report\n","              precision    recall  f1-score   support\n","\n","           0       0.93      0.81      0.87        84\n","           1       0.83      0.94      0.88        83\n","\n","    accuracy                           0.87       167\n","   macro avg       0.88      0.87      0.87       167\n","weighted avg       0.88      0.87      0.87       167\n","\n","Accuracy: 0.874251497005988\n"]}]},{"cell_type":"code","source":["#!kill -9 $(lsof -t)"],"metadata":{"id":"NOywiraTr2to","executionInfo":{"status":"ok","timestamp":1670062469686,"user_tz":-540,"elapsed":18,"user":{"displayName":"岩本陸","userId":"13347060052503904420"}}},"execution_count":6,"outputs":[]},{"cell_type":"code","source":["data = pd.read_csv(\"drive/My Drive/reserch/卒論/BERT-CNN-Fine-Tuning-For-Hate-Speech-Detection-in-Online-Social-Media-main/gunze_labeled_data.csv\")"],"metadata":{"id":"4fdjr2H4wheK","executionInfo":{"status":"ok","timestamp":1670062469687,"user_tz":-540,"elapsed":15,"user":{"displayName":"岩本陸","userId":"13347060052503904420"}}},"execution_count":7,"outputs":[]},{"cell_type":"code","source":["tokenizer = BertJapaneseTokenizer.from_pretrained('cl-tohoku/bert-base-japanese-whole-word-masking')\n","text = '東京都港区の大学で研究をしています'\n","input_ids = tokenizer.encode(text, return_tensors='pt')\n"," \n","print(input_ids)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"vvHE2e_L_cxG","executionInfo":{"status":"ok","timestamp":1670062470751,"user_tz":-540,"elapsed":1079,"user":{"displayName":"岩本陸","userId":"13347060052503904420"}},"outputId":"742aca1d-7db4-4364-987b-9c32c46ca214"},"execution_count":8,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([[   2,  391,  409, 1752,  280,    5,  396,   12,  426,   11,   15,   16,\n","           21, 2610,    3]])\n"]}]},{"cell_type":"code","source":["from sklearn.metrics import roc_auc_score\n","import numpy as np\n","\n","print(roc_auc_score(test_y, preds))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"LONS_cCxEWh1","executionInfo":{"status":"ok","timestamp":1670062470751,"user_tz":-540,"elapsed":14,"user":{"displayName":"岩本陸","userId":"13347060052503904420"}},"outputId":"cf6e28f4-5bed-4abf-a7fc-e7fed31ea1ae"},"execution_count":9,"outputs":[{"output_type":"stream","name":"stdout","text":["0.874641422834194\n"]}]},{"cell_type":"code","source":["from sklearn.metrics import confusion_matrix\n","\n","cm = confusion_matrix(test_y, preds)\n","cm"],"metadata":{"id":"OTzfWBG8ncF3","executionInfo":{"status":"ok","timestamp":1670062470752,"user_tz":-540,"elapsed":11,"user":{"displayName":"岩本陸","userId":"13347060052503904420"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"4db0dd1a-cb11-4d0e-cfc8-8d6b2a8debc7"},"execution_count":10,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([[68, 16],\n","       [ 5, 78]])"]},"metadata":{},"execution_count":10}]},{"cell_type":"code","source":["preds"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"iq4ZMEtRneai","executionInfo":{"status":"ok","timestamp":1670062470752,"user_tz":-540,"elapsed":10,"user":{"displayName":"岩本陸","userId":"13347060052503904420"}},"outputId":"e64ff3f9-e128-440b-e4f5-1eb905ec5fe6"},"execution_count":11,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0,\n","       1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1,\n","       1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1,\n","       1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0,\n","       0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0,\n","       1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1,\n","       0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1,\n","       1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0])"]},"metadata":{},"execution_count":11}]},{"cell_type":"code","source":["a = [-6.95692301e-02 -2.75502920e+00 -5.62757730e+00]\n","np.argmax(a)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"1aHodswCr6Vz","executionInfo":{"status":"ok","timestamp":1670062470752,"user_tz":-540,"elapsed":8,"user":{"displayName":"岩本陸","userId":"13347060052503904420"}},"outputId":"18797c3a-26bd-44cc-94de-b464ca708b10"},"execution_count":12,"outputs":[{"output_type":"execute_result","data":{"text/plain":["0"]},"metadata":{},"execution_count":12}]},{"cell_type":"code","source":["model"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"pCbftetdsDXW","executionInfo":{"status":"ok","timestamp":1670062470752,"user_tz":-540,"elapsed":7,"user":{"displayName":"岩本陸","userId":"13347060052503904420"}},"outputId":"c0dcd8ec-96fa-41eb-c250-ca52ab80f525"},"execution_count":13,"outputs":[{"output_type":"execute_result","data":{"text/plain":["BERT_Arch(\n","  (bert): BertModel(\n","    (embeddings): BertEmbeddings(\n","      (word_embeddings): Embedding(32000, 768, padding_idx=0)\n","      (position_embeddings): Embedding(512, 768)\n","      (token_type_embeddings): Embedding(2, 768)\n","      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","      (dropout): Dropout(p=0.1, inplace=False)\n","    )\n","    (encoder): BertEncoder(\n","      (layer): ModuleList(\n","        (0): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (1): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (2): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (3): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (4): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (5): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (6): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (7): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (8): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (9): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (10): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (11): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","      )\n","    )\n","    (pooler): BertPooler(\n","      (dense): Linear(in_features=768, out_features=768, bias=True)\n","      (activation): Tanh()\n","    )\n","  )\n","  (conv): Conv2d(13, 13, kernel_size=(3, 768), stride=(1, 1), padding=(1, 1), bias=False)\n","  (relu): ReLU()\n","  (pool): MaxPool2d(kernel_size=3, stride=1, padding=0, dilation=1, ceil_mode=False)\n","  (dropout): Dropout(p=0.1, inplace=False)\n","  (fc): Linear(in_features=1638, out_features=2, bias=True)\n","  (flat): Flatten(start_dim=1, end_dim=-1)\n","  (softmax): LogSoftmax(dim=1)\n",")"]},"metadata":{},"execution_count":13}]},{"cell_type":"code","source":[],"metadata":{"id":"0WiCWDfI8VY-","executionInfo":{"status":"ok","timestamp":1670062471334,"user_tz":-540,"elapsed":588,"user":{"displayName":"岩本陸","userId":"13347060052503904420"}}},"execution_count":13,"outputs":[]}]}